ROC Table Excercise
===============================
###################################################################################################
#OZKAN EMRE OZDEMIR
#HOMEWORK ASSIGNMENT #5 2/21/2016
###################################################################################################
#1.Training vs Test Data 
#a)Why are performance metrics better on training data than on test data?
# The model is generated by using the training data therefore the performance matrices can be slightly better than the test data.
# However, if the model is generated with 100 % accuracy using the training data, it may cause overfitting and further recudes 
# the accuracy of the performance metrics on test data.  	 
#
#b)How do you determine which data will be training data and which data will be test data?
# From All Data we randomly assign Holdout (Hidden) the  Test Data. The Data that is not Test Data is used for Training Data. 
#
#c)How do you determine which data were training data and which data were test data? 
# When we look at the performance metrics both the training data and test data, there should not be a significant difference to
# guess which data were used to build the model or which data is used to test the model. The model should be able to response to the
# test data with the similar accuracy as the training data. Otherwise, the model cannot be reliable.
###################################################################################################
#2.Beware, this problem contains irrelevant data while some important numbers are not explicitly presented. 
#A model was trained on 300 individuals where 149 had the cold and 151 were healthy. 
#The model was tested on 100 individuals where 10 were ill. 
#The model correctly predicted that 85 of the healthy individuals were indeed healthy 
#and correctly predicted that 7 of the ill individuals were indeed ill. 
#The other predictions were incorrect. 
#
#			True Condition				
#			  positive	negative			
# predicted	positive	7	5			
# conditions	negative	3	85			
#						
# TP	7		(a)	Sensitivity	TP/(TP+FN)		0.7
# TN	85		(b)	Specifity	TN/(FP+TN)		0.944444444
# FP	5		(c )	Accuracy	(TP+TN)/(TP+TN+FP+FN)	0.92
# FN	3		(d)	Precision	TP/(TP+FP)		0.583333333
# CN	90		(e )	Recall		TP/(TP+FN)		0.7
# CP	10					
#Consulted the Wikipedia page : http://en.wikipedia.org/wiki/Precision_and_recall 
###################################################################################################
#3.The probability threshold for a classification varies in an ROC chart from 0 to 1. 
#a) What point of the graph corresponds to a threshold of zero? 
#	Point (FPR = 1 , TPR = 1)
#b) What point of the graph corresponds to a threshold of one? 
#	Point (FPR = 0 , TPR = 0)
#c) What point of the graph corresponds to a threshold of 0.5? (trick question) 
#	Somewhere between (0, 0) and (1, 1) and it depends on the model probability predictions.
###################################################################################################
#4. A Classification is tested on 1000 cases. In the approximate middle of its ROC chart there is a point where the false positive
#rate is 0.4, the true positive rate is 0.8, and the accuracy is 0.7. 
#	FPR = 0.4 = FP/ FP + TN = 4 * 50 / 4 * 50 + 6 * 50
#	TPR = 0.8 = TP/ TP + FN = 8 *50 / 8*50 + 2 *50
#	Accuracy = 0.7 = (TP+TN)/(TP+TN+FP+FN) 
#a) What does the confusion matrix look like? 
#	TP = 400 	FP = 200
#	FN = 100	TN = 300
#	Accuracy = (TP+TN)/(TP+TN+FP+FN) = (400 + 300) /( 400 + 300 + 200 + 100) = 0.7
#b) What can you say about the probability threshold at that point? (trick question) 
#	I think, at this point all we can say about the threshold is, it is not 0 or 1.
###################################################################################################
